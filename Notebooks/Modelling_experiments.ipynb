{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02691b67",
   "metadata": {},
   "source": [
    "## Modelling experiments guidelines:\n",
    "\n",
    "1. The problem provided to us is a `Binary Classification problem`. The raw dataset is already pre-processed and saved. We will work with that processed dataset here for training different models and performing inference.\n",
    "\n",
    "2. Multiple models will be tried out here, ranging from baselines such as Logistic Regressions, KNN, SVM to tree-based ensembles like XgBoost, Random Forest, CatBoost etc to Neural Networks and Probabilistic models like Gaussian Processes (GP).\n",
    "\n",
    "3. One important thing to note is that the problem statement asked to train the model on entire dataset and save it. However, for our purpose, right now, due to the lack of test/unseen data, we will split our available data into training and test set, train models only on the training set and evaluate it on the unseen data. Later, the saved models will be trained on entire dataset, not on any split\n",
    "\n",
    "4. In this data, we will choose and report the best model. However the final models that will be saved in the `Models/` folder will be trained on the `entire dataset`.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcd7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91629\\Desktop\\Spring_Financial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2188, 101) (2188, 101) (2188, 103) (2188, 103)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "#   Load the Data path\n",
    "env_path = Path(os.getcwd()).parent / 'Config' / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "PREPROCESSED_PATH = os.getenv('PREPROCESSED_PATH')\n",
    "\n",
    "\n",
    "#   Load all the 4 datasets. They will be used for training different models as per convenience\n",
    "unprocessed_df = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'unprocessed_data.csv'))\n",
    "\n",
    "final_df_qt = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df.csv'))\n",
    "final_df_not_qt = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df_not_qt.csv'))\n",
    "\n",
    "qt_df_with_corr = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df_with_corr.csv'))\n",
    "not_qt_with_corr = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df_not_qt_with_corr.csv'))\n",
    "\n",
    "\n",
    "print(final_df_qt.shape, final_df_not_qt.shape, qt_df_with_corr.shape, not_qt_with_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ddcb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    1117\n",
      "0    1071\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#   The target variable is well-balanced. No need for any additional operations\n",
    "print(final_df_qt['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc38e9",
   "metadata": {},
   "source": [
    "To begin with, we will start with Baseline Models like Logistic Regression etc. For these models, the quantiled datasets like `final_df_qt` and `qt_df_with_corr` suits the best. We will do all our experiments with the `final_df_qt` and later repeat with the latter, keeping the same code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ad87e",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1daa7ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 100) (438, 100) (1750,) (438,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = final_df_qt.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.20, shuffle=True, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889745de",
   "metadata": {},
   "source": [
    "### Lets automate the model fitting, training and testing part:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b914cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a827da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_and_evaluate(X_train, X_test, y_train, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred, zero_division=0))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred, zero_division=0))\n",
    "    if auc is not None:\n",
    "        print(\"ROC AUC:\", auc)\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87d64",
   "metadata": {},
   "source": [
    "#### Train and evaluate on some baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff0df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Accuracy: 0.7123287671232876\n",
      "Precision: 0.6991150442477876\n",
      "Recall: 0.7314814814814815\n",
      "F1 Score: 0.7149321266968326\n",
      "ROC AUC: 0.7816358024691359\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.7123287671232876\n",
      "Precision: 0.6939655172413793\n",
      "Recall: 0.7453703703703703\n",
      "F1 Score: 0.71875\n",
      "ROC AUC: 0.7946800967634301\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.7214611872146118\n",
      "Precision: 0.7117117117117117\n",
      "Recall: 0.7314814814814815\n",
      "F1 Score: 0.7214611872146118\n",
      "ROC AUC: 0.7832311478144812\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.6894977168949772\n",
      "Precision: 0.6769911504424779\n",
      "Recall: 0.7083333333333334\n",
      "F1 Score: 0.6923076923076923\n",
      "ROC AUC: 0.7683725392058726\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.682648401826484\n",
      "Precision: 0.672645739910314\n",
      "Recall: 0.6944444444444444\n",
      "F1 Score: 0.683371298405467\n",
      "ROC AUC: 0.7606773440106774\n",
      "------------------------------\n",
      "Model: KNeighborsClassifier\n",
      "Accuracy: 0.6552511415525114\n",
      "Precision: 0.620817843866171\n",
      "Recall: 0.7731481481481481\n",
      "F1 Score: 0.688659793814433\n",
      "ROC AUC: 0.7135364531197865\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#   Seems like SVC(degree=2) and Logisitic regression are the top perfomers (around ~ 71% accuracy)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    SVC(kernel='rbf', probability=True),             \n",
    "    SVC(kernel='poly', degree=2, probability=True),\n",
    "    SVC(kernel='poly', degree=3, probability=True),  \n",
    "    SVC(kernel='poly', degree=4, probability=True),  \n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "# Run evaluation for all models\n",
    "for model in models:\n",
    "    model_fit_and_evaluate(X_train, X_test, y_train, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebba18",
   "metadata": {},
   "source": [
    "#### Now I want to check if our pre-processing even added any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42141695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 48) (438, 48) (1750,) (438,)\n"
     ]
    }
   ],
   "source": [
    "df = unprocessed_df.copy()\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(unprocessed_df.drop('target', axis=1), unprocessed_df['target'], test_size=0.20, shuffle=True, random_state=42)\n",
    "print(X_train_u.shape, X_test_u.shape, y_train_u.shape, y_test_u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91629\\Desktop\\Spring_Financial\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Accuracy: 0.7146118721461188\n",
      "Precision: 0.6842105263157895\n",
      "Recall: 0.7824074074074074\n",
      "F1 Score: 0.7300215982721382\n",
      "ROC AUC: 0.7743576910243577\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.684931506849315\n",
      "Precision: 0.6433823529411765\n",
      "Recall: 0.8101851851851852\n",
      "F1 Score: 0.7172131147540983\n",
      "ROC AUC: 0.7447238905572238\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.6164383561643836\n",
      "Precision: 0.5710059171597633\n",
      "Recall: 0.8935185185185185\n",
      "F1 Score: 0.6967509025270758\n",
      "ROC AUC: 0.6948302469135802\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.591324200913242\n",
      "Precision: 0.5506849315068493\n",
      "Recall: 0.9305555555555556\n",
      "F1 Score: 0.6919104991394148\n",
      "ROC AUC: 0.6536953620286955\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.5662100456621004\n",
      "Precision: 0.5347593582887701\n",
      "Recall: 0.9259259259259259\n",
      "F1 Score: 0.6779661016949152\n",
      "ROC AUC: 0.6158658658658659\n",
      "------------------------------\n",
      "Model: KNeighborsClassifier\n",
      "Accuracy: 0.6027397260273972\n",
      "Precision: 0.58203125\n",
      "Recall: 0.6898148148148148\n",
      "F1 Score: 0.6313559322033898\n",
      "ROC AUC: 0.6254796463129797\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    SVC(kernel='rbf', probability=True),             \n",
    "    SVC(kernel='poly', degree=2, probability=True),\n",
    "    SVC(kernel='poly', degree=3, probability=True),  \n",
    "    SVC(kernel='poly', degree=4, probability=True),  \n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model_fit_and_evaluate(X_train_u, X_test_u, y_train_u, y_test_u, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b4e80",
   "metadata": {},
   "source": [
    "**Comparing the scores, we can clearly see that our feature transformation has clearly improved the performance of our models. We can easily understand it from the `SVC results`. Moreover, convergence is now faster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5884f9",
   "metadata": {},
   "source": [
    "#### Lets use some combined_features now if they can improve the baselines (~71%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0cb4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=False, degree=2):\n",
    "    steps = []\n",
    "\n",
    "    if use_poly:\n",
    "        steps.append(('poly', PolynomialFeatures(degree=degree)))\n",
    "    \n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    result =  {\n",
    "        'Model': model.__class__.__name__ + (f' (poly deg={degree})' if use_poly else ''),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC AUC': auc\n",
    "    }\n",
    "\n",
    "    if isinstance(model, SVC):\n",
    "        result.update({\n",
    "            'kernel': model.kernel,\n",
    "            'degree': model.degree,\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3edad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with Linear models....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:19<00:00, 69.81s/it]\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>kernel</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.721461</td>\n",
       "      <td>0.711712</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.721461</td>\n",
       "      <td>0.783262</td>\n",
       "      <td>poly</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.714612</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.716553</td>\n",
       "      <td>0.780197</td>\n",
       "      <td>linear</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.699115</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.714932</td>\n",
       "      <td>0.781636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.693966</td>\n",
       "      <td>0.745370</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.794680</td>\n",
       "      <td>rbf</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.689498</td>\n",
       "      <td>0.676991</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.768456</td>\n",
       "      <td>poly</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.682648</td>\n",
       "      <td>0.672646</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.683371</td>\n",
       "      <td>0.760761</td>\n",
       "      <td>poly</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression (poly deg=2)</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>0.646018</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.660633</td>\n",
       "      <td>0.697823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.655251</td>\n",
       "      <td>0.620818</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.688660</td>\n",
       "      <td>0.713536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC (poly deg=2)</td>\n",
       "      <td>0.646119</td>\n",
       "      <td>0.639269</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.676885</td>\n",
       "      <td>linear</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy  Precision    Recall  F1 Score  \\\n",
       "5                              SVC  0.721461   0.711712  0.731481  0.721461   \n",
       "2                              SVC  0.714612   0.702222  0.731481  0.716553   \n",
       "0               LogisticRegression  0.712329   0.699115  0.731481  0.714932   \n",
       "4                              SVC  0.712329   0.693966  0.745370  0.718750   \n",
       "6                              SVC  0.689498   0.676991  0.708333  0.692308   \n",
       "7                              SVC  0.682648   0.672646  0.694444  0.683371   \n",
       "1  LogisticRegression (poly deg=2)  0.657534   0.646018  0.675926  0.660633   \n",
       "8             KNeighborsClassifier  0.655251   0.620818  0.773148  0.688660   \n",
       "3                 SVC (poly deg=2)  0.646119   0.639269  0.648148  0.643678   \n",
       "\n",
       "    ROC AUC  kernel  degree  \n",
       "5  0.783262    poly     2.0  \n",
       "2  0.780197  linear     3.0  \n",
       "0  0.781636     NaN     NaN  \n",
       "4  0.794680     rbf     3.0  \n",
       "6  0.768456    poly     3.0  \n",
       "7  0.760761    poly     4.0  \n",
       "1  0.697823     NaN     NaN  \n",
       "8  0.713536     NaN     NaN  \n",
       "3  0.676885  linear     3.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Combined_feature was able to outperform the baseline (although by a mere margin). \n",
    "#   The current percent stands at ~ 72%\n",
    "\n",
    "results = []\n",
    "\n",
    "linear_models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    SVC(kernel='linear', probability=True)\n",
    "]\n",
    "\n",
    "nonlinear_models = [\n",
    "    SVC(kernel='rbf', probability=True),\n",
    "    SVC(kernel='poly', degree=2, probability=True),\n",
    "    SVC(kernel='poly', degree=3, probability=True),\n",
    "    SVC(kernel='poly', degree=4, probability=True),\n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "# Linear models with and without poly\n",
    "print('Starting with Linear models....')\n",
    "for model in tqdm(linear_models):\n",
    "    results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=False))     #   ~ 100 features\n",
    "    results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=True, degree=2))  # ~10000 features\n",
    "    # results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=True, degree=3))\n",
    "\n",
    "\n",
    "# Non-linear models without poly\n",
    "for model in tqdm(nonlinear_models):\n",
    "    results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=False))\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40459bfc",
   "metadata": {},
   "source": [
    "**Thus, among our baselines, `SVC` with `polynomial kernel` and `degree=2` provided us with the best results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772a93a",
   "metadata": {},
   "source": [
    "1. Now, we will move to the tree-based ensemble methods, such as RandomForest, XgBoost, LightGBM, CatBoost etc. An important thing to note now is that, for model-fitting on tree based methods, data transformations don't tend to work well. In fact, they are known to detoriate performace.\n",
    "\n",
    "2. That's why, we will be using the `final_df_not_qt` as our data here. Notably, the (numerical + other) features are not quantiled here. Also, the remaining features are also not standardized; perfect for fitting tree-based models.\n",
    "\n",
    "3. Also, it is not recommended to create polynomial features for tree-based ensembles. The models are capable of capturing interactions and nonlinearities on their own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480803d1",
   "metadata": {},
   "source": [
    "#### Performing train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7612fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 100) (438, 100) (1750,) (438,)\n"
     ]
    }
   ],
   "source": [
    "df = final_df_not_qt.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.20, shuffle=True, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc699ff",
   "metadata": {},
   "source": [
    "#### Tree-based ensemble model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455298a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tree_model(X_train, X_test, y_train, y_test, model, param_desc=''):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    return {\n",
    "        'Model': model.__class__.__name__,\n",
    "        'Params': param_desc,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC AUC': auc\n",
    "    }\n",
    "\n",
    "def run_tree_model_grid(X_train, X_test, y_train, y_test):\n",
    "    results = []\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'n_estimators': [250, 450, 750],\n",
    "        'learning_rate': [0.05, 0.1],  # Only for boosting models\n",
    "    }\n",
    "\n",
    "    rf_params = [(d, n) for d in param_grid['max_depth'] for n in param_grid['n_estimators']]\n",
    "    boosting_params = [(d, n, lr) for d in param_grid['max_depth'] for n in param_grid['n_estimators'] for lr in param_grid['learning_rate']]\n",
    "\n",
    "    print(\"Running Random Forest...\")\n",
    "    for d, n in tqdm(rf_params):\n",
    "        model = RandomForestClassifier(max_depth=d, n_estimators=n, random_state=42)\n",
    "        desc = f'max_depth={d}, n_estimators={n}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    print(\"Running XGBoost...\")\n",
    "    for d, n, lr in tqdm(boosting_params):\n",
    "        model = xgb.XGBClassifier(max_depth=d, n_estimators=n, learning_rate=lr,\n",
    "                                   eval_metric='logloss', random_state=42)\n",
    "        desc = f'max_depth={d}, n_estimators={n}, lr={lr}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    print(\"Running LightGBM...\")\n",
    "    for d, n, lr in tqdm(boosting_params):\n",
    "        model = lgb.LGBMClassifier(max_depth=d, n_estimators=n, learning_rate=lr,\n",
    "                                force_col_wise=True, verbosity=-1, random_state=42)\n",
    "        desc = f'max_depth={d}, n_estimators={n}, lr={lr}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    print(\"Running CatBoost...\")\n",
    "    for d, n, lr in tqdm(boosting_params):\n",
    "        model = CatBoostClassifier(depth=d, iterations=n, learning_rate=lr,\n",
    "                                   verbose=0, random_state=42)\n",
    "        desc = f'depth={d}, iterations={n}, lr={lr}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55c5f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:16<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:41<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:27<00:00,  4.87s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Params</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>depth=3, iterations=250, lr=0.05</td>\n",
       "      <td>0.751142</td>\n",
       "      <td>0.735683</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.753950</td>\n",
       "      <td>0.813167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>max_depth=3, n_estimators=250, lr=0.05</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.736607</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.806953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>max_depth=7, n_estimators=250</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>0.810540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=3, n_estimators=250, lr=0.05</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>0.808037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>max_depth=5, n_estimators=450</td>\n",
       "      <td>0.746575</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.805535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=3, n_estimators=750, lr=0.1</td>\n",
       "      <td>0.710046</td>\n",
       "      <td>0.692641</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.715884</td>\n",
       "      <td>0.795796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=7, n_estimators=450, lr=0.1</td>\n",
       "      <td>0.707763</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.711712</td>\n",
       "      <td>0.799383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>depth=5, iterations=750, lr=0.1</td>\n",
       "      <td>0.707763</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.799383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=5, n_estimators=450, lr=0.1</td>\n",
       "      <td>0.705479</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.711409</td>\n",
       "      <td>0.795942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=5, n_estimators=750, lr=0.1</td>\n",
       "      <td>0.700913</td>\n",
       "      <td>0.683983</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.706935</td>\n",
       "      <td>0.792918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model                                  Params  Accuracy  \\\n",
       "45      CatBoostClassifier        depth=3, iterations=250, lr=0.05  0.751142   \n",
       "9            XGBClassifier  max_depth=3, n_estimators=250, lr=0.05  0.748858   \n",
       "6   RandomForestClassifier           max_depth=7, n_estimators=250  0.748858   \n",
       "27          LGBMClassifier  max_depth=3, n_estimators=250, lr=0.05  0.748858   \n",
       "4   RandomForestClassifier           max_depth=5, n_estimators=450  0.746575   \n",
       "..                     ...                                     ...       ...   \n",
       "32          LGBMClassifier   max_depth=3, n_estimators=750, lr=0.1  0.710046   \n",
       "42          LGBMClassifier   max_depth=7, n_estimators=450, lr=0.1  0.707763   \n",
       "56      CatBoostClassifier         depth=5, iterations=750, lr=0.1  0.707763   \n",
       "36          LGBMClassifier   max_depth=5, n_estimators=450, lr=0.1  0.705479   \n",
       "38          LGBMClassifier   max_depth=5, n_estimators=750, lr=0.1  0.700913   \n",
       "\n",
       "    Precision    Recall  F1 Score   ROC AUC  \n",
       "45   0.735683  0.773148  0.753950  0.813167  \n",
       "9    0.736607  0.763889  0.750000  0.806953  \n",
       "6    0.734513  0.768519  0.751131  0.810540  \n",
       "27   0.734513  0.768519  0.751131  0.808037  \n",
       "4    0.733333  0.763889  0.748299  0.805535  \n",
       "..        ...       ...       ...       ...  \n",
       "32   0.692641  0.740741  0.715884  0.795796  \n",
       "42   0.692982  0.731481  0.711712  0.799383  \n",
       "56   0.703704  0.703704  0.703704  0.799383  \n",
       "36   0.688312  0.736111  0.711409  0.795942  \n",
       "38   0.683983  0.731481  0.706935  0.792918  \n",
       "\n",
       "[63 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tree_results = run_tree_model_grid(X_train, X_test, y_train, y_test)\n",
    "df_tree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b71d0",
   "metadata": {},
   "source": [
    "**From the above evaluations, it is clear that CatBoost outperformed the previous baseline and now the best performing model (CatBoost) stands at accuracy ~ 75% !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9698b",
   "metadata": {},
   "source": [
    "### Gaussian Processes:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e958e33",
   "metadata": {},
   "source": [
    "1. Now, we will move to a new type of model, the Probabilistic models. We saw in the pre-processing section that after transformation, most of the (numerical + other) features were approximately Gaussian in their distribution.\n",
    "\n",
    "2. To fit these distributions, Gaussian Processes (GP) can be a good choice since they are non-parametric and based on Gaussian distributions. Along with predicting the class, they are also able to predict the uncertainity estimate for that prediction.\n",
    "\n",
    "3. We can use a hybrid kernel design (also combining with ARD kernel). This gives us natural feature selection and ability to handle both numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f01b9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeKernelGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, num_numerical, num_binary):\n",
    "        # Variational distribution and strategy\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(0)\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        \n",
    "     \n",
    "        self.num_numerical = num_numerical\n",
    "        self.num_binary = num_binary\n",
    "     \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        # Composite kernel:\n",
    "        # 1. ARD-RBF for numerical features\n",
    "        # 2. Matern (ν=0.5) for binary features\n",
    "        self.covar_module = (\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(\n",
    "                    ard_num_dims=num_numerical,\n",
    "                    active_dims=range(num_numerical)\n",
    "                )\n",
    "                + gpytorch.kernels.ScaleKernel(\n",
    "                    gpytorch.kernels.MaternKernel(\n",
    "                        nu=0.5,\n",
    "                        ard_num_dims=num_binary,\n",
    "                        active_dims=range(num_numerical, num_numerical + num_binary)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpytorch_model(X_train, y_train, num_numerical, num_binary, num_epochs=100, lr=0.1):\n",
    "    \n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train = X_train.values\n",
    "\n",
    "    if isinstance(y_train, pd.DataFrame):\n",
    "        y_train = y_train.values\n",
    "    \n",
    "\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "    \n",
    "    num_inducing = max(100, len(X_train) // 10)\n",
    "    rand_idx = torch.randperm(len(X_train))[:num_inducing]\n",
    "    inducing_points = X_train_t[rand_idx].clone()\n",
    "    \n",
    "   \n",
    "    model = CompositeKernelGP(\n",
    "        inducing_points=inducing_points,\n",
    "        num_numerical=num_numerical,\n",
    "        num_binary=num_binary\n",
    "    )\n",
    "    likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()}\n",
    "    ], lr=lr)\n",
    "    \n",
    "    \n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y_train_t.size(0))\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_t)\n",
    "        loss = -mll(output, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_gpytorch(model, likelihood, X_test, y_test):\n",
    "\n",
    "    if isinstance(X_test, pd.DataFrame):\n",
    "        X_test = X_test.values\n",
    "\n",
    "    if isinstance(y_test, pd.DataFrame):\n",
    "        y_test = y_test.values\n",
    "\n",
    "\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "   \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(X_test_t))\n",
    "        probabilities = observed_pred.mean.numpy()\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "    \n",
    "   \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, predictions),\n",
    "        'Precision': precision_score(y_test, predictions, zero_division=0),\n",
    "        'Recall': recall_score(y_test, predictions, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, predictions, zero_division=0),\n",
    "        'ROC AUC': roc_auc_score(y_test, probabilities)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def gp_model_fit_and_evaluate(X_train, X_test, y_train, y_test, num_numerical, num_binary):\n",
    "\n",
    "    X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    X_test = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "    y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "    y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "\n",
    "    model, likelihood = train_gpytorch_model(\n",
    "        X_train, y_train,\n",
    "        num_numerical=num_numerical,\n",
    "        num_binary=num_binary\n",
    "    )\n",
    "    \n",
    " \n",
    "    metrics = evaluate_gpytorch(model, likelihood, X_test, y_test)\n",
    "    \n",
    "    top_scale = model.covar_module\n",
    "    sum_kernel = top_scale.base_kernel\n",
    "    rbf_kernel = sum_kernel.kernels[0]\n",
    "    matern_scale = sum_kernel.kernels[1]\n",
    "    matern_kernel = matern_scale.base_kernel\n",
    "\n",
    "    lengthscales_numerical = rbf_kernel.lengthscale.detach().cpu().numpy().squeeze()\n",
    "    lengthscales_binary   = matern_kernel.lengthscale.detach().cpu().numpy().squeeze()\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'likelihood': likelihood,\n",
    "        'metrics': metrics,\n",
    "        'lengthscales_numerical': lengthscales_numerical,\n",
    "        'lengthscales_binary': lengthscales_binary\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:08, 10.78it/s]c:\\Users\\91629\\Desktop\\Spring_Financial\\.venv\\Lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
      "  warnings.warn(\n",
      " 11%|█         | 11/100 [00:01<00:08, 10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Loss: 0.7494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:02<00:07, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Loss: 0.6676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:03<00:06, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Loss: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:03<00:05, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 - Loss: 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:04<00:04, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Loss: 0.6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:05<00:02, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 - Loss: 0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:06<00:02, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 - Loss: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:07<00:01, 13.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 - Loss: 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [00:07<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 - Loss: 0.6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 - Loss: 0.6328\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.6256\n",
      "Precision: 0.6016\n",
      "Recall: 0.7130\n",
      "F1 Score: 0.6525\n",
      "ROC AUC: 0.6743\n",
      "\n",
      "Feature Importance (Numerical - smaller lengthscale = more important):\n",
      "Numerical features sorted by importance (smaller lengthscale ⇒ more important):\n",
      " 1. Feature  16 — lengthscale = 0.3656\n",
      " 2. Feature  15 — lengthscale = 0.4912\n",
      " 3. Feature   4 — lengthscale = 0.5693\n",
      " 4. Feature  23 — lengthscale = 0.9318\n",
      " 5. Feature  22 — lengthscale = 0.9826\n",
      " 6. Feature  20 — lengthscale = 1.0878\n",
      " 7. Feature   3 — lengthscale = 1.0880\n",
      " 8. Feature  13 — lengthscale = 1.6766\n",
      " 9. Feature  12 — lengthscale = 2.0406\n",
      "10. Feature   7 — lengthscale = 2.3662\n",
      "11. Feature   8 — lengthscale = 2.3838\n",
      "12. Feature  14 — lengthscale = 2.4361\n",
      "13. Feature  18 — lengthscale = 2.4938\n",
      "14. Feature   1 — lengthscale = 2.5650\n",
      "15. Feature  24 — lengthscale = 2.5737\n",
      "16. Feature  10 — lengthscale = 2.6588\n",
      "17. Feature  17 — lengthscale = 2.8146\n",
      "18. Feature  11 — lengthscale = 2.8482\n",
      "19. Feature  25 — lengthscale = 2.9044\n",
      "20. Feature  19 — lengthscale = 2.9529\n",
      "21. Feature   6 — lengthscale = 2.9560\n",
      "22. Feature  21 — lengthscale = 3.2500\n",
      "23. Feature   5 — lengthscale = 3.3888\n",
      "24. Feature   2 — lengthscale = 3.5039\n",
      "25. Feature   9 — lengthscale = 3.5060\n",
      "26. Feature  26 — lengthscale = 3.6815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = gp_model_fit_and_evaluate(\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    num_numerical=101-74-1,\n",
    "    num_binary=74\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nFeature Importance (Numerical - smaller lengthscale = more important):\")\n",
    "ls_num = results['lengthscales_numerical'].squeeze()  # shape: (num_numerical,)\n",
    "\n",
    "#   Sorted by importance\n",
    "sorted_idxs = np.argsort(ls_num)\n",
    "print(\"Numerical features sorted by importance (smaller lengthscale ⇒ more important):\")\n",
    "for rank, idx in enumerate(sorted_idxs, start=1):\n",
    "    print(f\"{rank:2d}. Feature {idx+1:3d} — lengthscale = {ls_num[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe4111",
   "metadata": {},
   "source": [
    "**Although the Gaussian Process model was not able to outperform the Tree-based ensemble models, yet it provided us with a very important insight => Feature Importance...!! This can help understand very clearly which feature has more predictive power than the other**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13189111",
   "metadata": {},
   "source": [
    "### Neural Networks:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df7bb4",
   "metadata": {},
   "source": [
    "1. We will try out 2-3 different types of Neural Network architectures and then check how does they perform as compared to the previous models and baselines\n",
    "\n",
    "2. And, for neural networks as well, we will use the quantile transformed dataset as neural networks tend to fit well on transformed and standardized datasets..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632148ad",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fca6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 100) (438, 100) (1750,) (438,)\n"
     ]
    }
   ],
   "source": [
    "df = final_df_qt.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.20, shuffle=True, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c96fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93a672aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Type-I architecture\n",
    "class Network_v1(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1),\n",
    "            # nn.Sigmoid()          #   We will use BCEWithLogits loss (to induce more stability)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "\n",
    "class Network_v2(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1),\n",
    "            # nn.Sigmoid()          #   We will use BCEWithLogits loss (to induce more stability)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "\n",
    "#   Using skip-connection\n",
    "class Network_v3(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 100),\n",
    "            nn.SiLU()\n",
    "            # nn.Sigmoid()          #   We will use BCEWithLogits loss (to induce more stability)\n",
    "        )\n",
    "\n",
    "        self.layers2 = nn.Sequential(\n",
    "            nn.Linear(100, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1),\n",
    "            # nn.Sigmoid()          #   We will use BCEWithLogits loss (to induce more stability)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = self.layers1(x)\n",
    "        return self.layers2(x + skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4673489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 32,\n",
    "    device: str = None\n",
    "):\n",
    "    \n",
    "    device = 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    if not torch.is_tensor(X_train):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    if not torch.is_tensor(y_train):\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    test_ds = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "  \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(yb.numpy().flatten().tolist())\n",
    "\n",
    "    \n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    prec = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probs)\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "\n",
    "    print(\"\\nEvaluation Results\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"ROC AUC: {auc:.4f}\")\n",
    "    print('-' * 30)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db38141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def run_all_models(X_train, y_train, X_test, y_test, epochs=70, batch_size=64):\n",
    "    models = [\n",
    "        ('Network_v1', Network_v1()),\n",
    "        ('Network_v2', Network_v2()),\n",
    "        ('Network_v3', Network_v3())\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models:\n",
    "        print(f\"\\n=== Training {name} ===\")\n",
    "        optimizer = AdamW(model.parameters())\n",
    "        metrics = train_and_evaluate(\n",
    "            X_train, y_train,\n",
    "            X_test,  y_test,\n",
    "            model,\n",
    "            optimizer,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        results[name] = metrics\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ffb9b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Network_v1 ===\n",
      "Epoch 1/70 - Loss: 0.6749\n",
      "Epoch 2/70 - Loss: 0.6128\n",
      "Epoch 3/70 - Loss: 0.5544\n",
      "Epoch 4/70 - Loss: 0.5308\n",
      "Epoch 5/70 - Loss: 0.5223\n",
      "Epoch 6/70 - Loss: 0.5200\n",
      "Epoch 7/70 - Loss: 0.5137\n",
      "Epoch 8/70 - Loss: 0.5082\n",
      "Epoch 9/70 - Loss: 0.5082\n",
      "Epoch 10/70 - Loss: 0.5047\n",
      "Epoch 11/70 - Loss: 0.4984\n",
      "Epoch 12/70 - Loss: 0.4966\n",
      "Epoch 13/70 - Loss: 0.4918\n",
      "Epoch 14/70 - Loss: 0.4862\n",
      "Epoch 15/70 - Loss: 0.4811\n",
      "Epoch 16/70 - Loss: 0.4800\n",
      "Epoch 17/70 - Loss: 0.4757\n",
      "Epoch 18/70 - Loss: 0.4731\n",
      "Epoch 19/70 - Loss: 0.4598\n",
      "Epoch 20/70 - Loss: 0.4578\n",
      "Epoch 21/70 - Loss: 0.4526\n",
      "Epoch 22/70 - Loss: 0.4540\n",
      "Epoch 23/70 - Loss: 0.4406\n",
      "Epoch 24/70 - Loss: 0.4362\n",
      "Epoch 25/70 - Loss: 0.4337\n",
      "Epoch 26/70 - Loss: 0.4283\n",
      "Epoch 27/70 - Loss: 0.4238\n",
      "Epoch 28/70 - Loss: 0.4186\n",
      "Epoch 29/70 - Loss: 0.4067\n",
      "Epoch 30/70 - Loss: 0.4045\n",
      "Epoch 31/70 - Loss: 0.3991\n",
      "Epoch 32/70 - Loss: 0.3902\n",
      "Epoch 33/70 - Loss: 0.3986\n",
      "Epoch 34/70 - Loss: 0.3830\n",
      "Epoch 35/70 - Loss: 0.3827\n",
      "Epoch 36/70 - Loss: 0.3757\n",
      "Epoch 37/70 - Loss: 0.3691\n",
      "Epoch 38/70 - Loss: 0.3631\n",
      "Epoch 39/70 - Loss: 0.3555\n",
      "Epoch 40/70 - Loss: 0.3511\n",
      "Epoch 41/70 - Loss: 0.3457\n",
      "Epoch 42/70 - Loss: 0.3395\n",
      "Epoch 43/70 - Loss: 0.3396\n",
      "Epoch 44/70 - Loss: 0.3345\n",
      "Epoch 45/70 - Loss: 0.3214\n",
      "Epoch 46/70 - Loss: 0.3208\n",
      "Epoch 47/70 - Loss: 0.3235\n",
      "Epoch 48/70 - Loss: 0.3097\n",
      "Epoch 49/70 - Loss: 0.3068\n",
      "Epoch 50/70 - Loss: 0.2983\n",
      "Epoch 51/70 - Loss: 0.3036\n",
      "Epoch 52/70 - Loss: 0.2948\n",
      "Epoch 53/70 - Loss: 0.2913\n",
      "Epoch 54/70 - Loss: 0.2865\n",
      "Epoch 55/70 - Loss: 0.2929\n",
      "Epoch 56/70 - Loss: 0.2817\n",
      "Epoch 57/70 - Loss: 0.2733\n",
      "Epoch 58/70 - Loss: 0.2710\n",
      "Epoch 59/70 - Loss: 0.2618\n",
      "Epoch 60/70 - Loss: 0.2548\n",
      "Epoch 61/70 - Loss: 0.2581\n",
      "Epoch 62/70 - Loss: 0.2570\n",
      "Epoch 63/70 - Loss: 0.2430\n",
      "Epoch 64/70 - Loss: 0.2352\n",
      "Epoch 65/70 - Loss: 0.2369\n",
      "Epoch 66/70 - Loss: 0.2365\n",
      "Epoch 67/70 - Loss: 0.2251\n",
      "Epoch 68/70 - Loss: 0.2264\n",
      "Epoch 69/70 - Loss: 0.2129\n",
      "Epoch 70/70 - Loss: 0.2150\n",
      "\n",
      "Evaluation Results\n",
      "Accuracy: 0.6826\n",
      "Precision: 0.6611\n",
      "Recall: 0.7315\n",
      "F1 Score: 0.6945\n",
      "ROC AUC: 0.7265\n",
      "------------------------------\n",
      "\n",
      "=== Training Network_v2 ===\n",
      "Epoch 1/70 - Loss: 0.6845\n",
      "Epoch 2/70 - Loss: 0.6292\n",
      "Epoch 3/70 - Loss: 0.5587\n",
      "Epoch 4/70 - Loss: 0.5432\n",
      "Epoch 5/70 - Loss: 0.5304\n",
      "Epoch 6/70 - Loss: 0.5234\n",
      "Epoch 7/70 - Loss: 0.5185\n",
      "Epoch 8/70 - Loss: 0.5106\n",
      "Epoch 9/70 - Loss: 0.5092\n",
      "Epoch 10/70 - Loss: 0.5050\n",
      "Epoch 11/70 - Loss: 0.5006\n",
      "Epoch 12/70 - Loss: 0.4901\n",
      "Epoch 13/70 - Loss: 0.4923\n",
      "Epoch 14/70 - Loss: 0.4907\n",
      "Epoch 15/70 - Loss: 0.4827\n",
      "Epoch 16/70 - Loss: 0.4769\n",
      "Epoch 17/70 - Loss: 0.4758\n",
      "Epoch 18/70 - Loss: 0.4656\n",
      "Epoch 19/70 - Loss: 0.4576\n",
      "Epoch 20/70 - Loss: 0.4541\n",
      "Epoch 21/70 - Loss: 0.4478\n",
      "Epoch 22/70 - Loss: 0.4425\n",
      "Epoch 23/70 - Loss: 0.4384\n",
      "Epoch 24/70 - Loss: 0.4220\n",
      "Epoch 25/70 - Loss: 0.4172\n",
      "Epoch 26/70 - Loss: 0.4109\n",
      "Epoch 27/70 - Loss: 0.4108\n",
      "Epoch 28/70 - Loss: 0.4029\n",
      "Epoch 29/70 - Loss: 0.3950\n",
      "Epoch 30/70 - Loss: 0.3859\n",
      "Epoch 31/70 - Loss: 0.3878\n",
      "Epoch 32/70 - Loss: 0.3839\n",
      "Epoch 33/70 - Loss: 0.3740\n",
      "Epoch 34/70 - Loss: 0.3644\n",
      "Epoch 35/70 - Loss: 0.3569\n",
      "Epoch 36/70 - Loss: 0.3525\n",
      "Epoch 37/70 - Loss: 0.3433\n",
      "Epoch 38/70 - Loss: 0.3383\n",
      "Epoch 39/70 - Loss: 0.3274\n",
      "Epoch 40/70 - Loss: 0.3338\n",
      "Epoch 41/70 - Loss: 0.3158\n",
      "Epoch 42/70 - Loss: 0.3090\n",
      "Epoch 43/70 - Loss: 0.3075\n",
      "Epoch 44/70 - Loss: 0.3102\n",
      "Epoch 45/70 - Loss: 0.3016\n",
      "Epoch 46/70 - Loss: 0.2984\n",
      "Epoch 47/70 - Loss: 0.2993\n",
      "Epoch 48/70 - Loss: 0.2936\n",
      "Epoch 49/70 - Loss: 0.2986\n",
      "Epoch 50/70 - Loss: 0.2653\n",
      "Epoch 51/70 - Loss: 0.2551\n",
      "Epoch 52/70 - Loss: 0.2660\n",
      "Epoch 53/70 - Loss: 0.2591\n",
      "Epoch 54/70 - Loss: 0.2571\n",
      "Epoch 55/70 - Loss: 0.2520\n",
      "Epoch 56/70 - Loss: 0.2609\n",
      "Epoch 57/70 - Loss: 0.2452\n",
      "Epoch 58/70 - Loss: 0.2368\n",
      "Epoch 59/70 - Loss: 0.2306\n",
      "Epoch 60/70 - Loss: 0.2434\n",
      "Epoch 61/70 - Loss: 0.2313\n",
      "Epoch 62/70 - Loss: 0.2223\n",
      "Epoch 63/70 - Loss: 0.2186\n",
      "Epoch 64/70 - Loss: 0.2128\n",
      "Epoch 65/70 - Loss: 0.2166\n",
      "Epoch 66/70 - Loss: 0.2059\n",
      "Epoch 67/70 - Loss: 0.2190\n",
      "Epoch 68/70 - Loss: 0.2133\n",
      "Epoch 69/70 - Loss: 0.2150\n",
      "Epoch 70/70 - Loss: 0.2083\n",
      "\n",
      "Evaluation Results\n",
      "Accuracy: 0.6598\n",
      "Precision: 0.6402\n",
      "Recall: 0.7083\n",
      "F1 Score: 0.6725\n",
      "ROC AUC: 0.7073\n",
      "------------------------------\n",
      "\n",
      "=== Training Network_v3 ===\n",
      "Epoch 1/70 - Loss: 0.6860\n",
      "Epoch 2/70 - Loss: 0.6289\n",
      "Epoch 3/70 - Loss: 0.5538\n",
      "Epoch 4/70 - Loss: 0.5335\n",
      "Epoch 5/70 - Loss: 0.5268\n",
      "Epoch 6/70 - Loss: 0.5195\n",
      "Epoch 7/70 - Loss: 0.5130\n",
      "Epoch 8/70 - Loss: 0.5074\n",
      "Epoch 9/70 - Loss: 0.5007\n",
      "Epoch 10/70 - Loss: 0.4965\n",
      "Epoch 11/70 - Loss: 0.5013\n",
      "Epoch 12/70 - Loss: 0.4881\n",
      "Epoch 13/70 - Loss: 0.4900\n",
      "Epoch 14/70 - Loss: 0.4824\n",
      "Epoch 15/70 - Loss: 0.4800\n",
      "Epoch 16/70 - Loss: 0.4664\n",
      "Epoch 17/70 - Loss: 0.4634\n",
      "Epoch 18/70 - Loss: 0.4557\n",
      "Epoch 19/70 - Loss: 0.4585\n",
      "Epoch 20/70 - Loss: 0.4465\n",
      "Epoch 21/70 - Loss: 0.4383\n",
      "Epoch 22/70 - Loss: 0.4313\n",
      "Epoch 23/70 - Loss: 0.4208\n",
      "Epoch 24/70 - Loss: 0.4282\n",
      "Epoch 25/70 - Loss: 0.4131\n",
      "Epoch 26/70 - Loss: 0.4065\n",
      "Epoch 27/70 - Loss: 0.3964\n",
      "Epoch 28/70 - Loss: 0.3967\n",
      "Epoch 29/70 - Loss: 0.3857\n",
      "Epoch 30/70 - Loss: 0.3738\n",
      "Epoch 31/70 - Loss: 0.3694\n",
      "Epoch 32/70 - Loss: 0.3765\n",
      "Epoch 33/70 - Loss: 0.3533\n",
      "Epoch 34/70 - Loss: 0.3498\n",
      "Epoch 35/70 - Loss: 0.3565\n",
      "Epoch 36/70 - Loss: 0.3404\n",
      "Epoch 37/70 - Loss: 0.3337\n",
      "Epoch 38/70 - Loss: 0.3304\n",
      "Epoch 39/70 - Loss: 0.3233\n",
      "Epoch 40/70 - Loss: 0.3087\n",
      "Epoch 41/70 - Loss: 0.3090\n",
      "Epoch 42/70 - Loss: 0.3050\n",
      "Epoch 43/70 - Loss: 0.2927\n",
      "Epoch 44/70 - Loss: 0.2898\n",
      "Epoch 45/70 - Loss: 0.2849\n",
      "Epoch 46/70 - Loss: 0.2673\n",
      "Epoch 47/70 - Loss: 0.2822\n",
      "Epoch 48/70 - Loss: 0.2863\n",
      "Epoch 49/70 - Loss: 0.2567\n",
      "Epoch 50/70 - Loss: 0.2609\n",
      "Epoch 51/70 - Loss: 0.2819\n",
      "Epoch 52/70 - Loss: 0.2479\n",
      "Epoch 53/70 - Loss: 0.2476\n",
      "Epoch 54/70 - Loss: 0.2391\n",
      "Epoch 55/70 - Loss: 0.2398\n",
      "Epoch 56/70 - Loss: 0.2533\n",
      "Epoch 57/70 - Loss: 0.2229\n",
      "Epoch 58/70 - Loss: 0.2170\n",
      "Epoch 59/70 - Loss: 0.2187\n",
      "Epoch 60/70 - Loss: 0.2245\n",
      "Epoch 61/70 - Loss: 0.2211\n",
      "Epoch 62/70 - Loss: 0.2073\n",
      "Epoch 63/70 - Loss: 0.2013\n",
      "Epoch 64/70 - Loss: 0.1941\n",
      "Epoch 65/70 - Loss: 0.2102\n",
      "Epoch 66/70 - Loss: 0.2256\n",
      "Epoch 67/70 - Loss: 0.1896\n",
      "Epoch 68/70 - Loss: 0.2140\n",
      "Epoch 69/70 - Loss: 0.1807\n",
      "Epoch 70/70 - Loss: 0.1842\n",
      "\n",
      "Evaluation Results\n",
      "Accuracy: 0.6804\n",
      "Precision: 0.6496\n",
      "Recall: 0.7639\n",
      "F1 Score: 0.7021\n",
      "ROC AUC: 0.7332\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Network_v1</th>\n",
       "      <th>Network_v2</th>\n",
       "      <th>Network_v3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.682648</td>\n",
       "      <td>0.659817</td>\n",
       "      <td>0.680365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.661088</td>\n",
       "      <td>0.640167</td>\n",
       "      <td>0.649606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.763889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.694505</td>\n",
       "      <td>0.672527</td>\n",
       "      <td>0.702128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.726456</td>\n",
       "      <td>0.707332</td>\n",
       "      <td>0.733192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Network_v1  Network_v2  Network_v3\n",
       "accuracy     0.682648    0.659817    0.680365\n",
       "precision    0.661088    0.640167    0.649606\n",
       "recall       0.731481    0.708333    0.763889\n",
       "f1_score     0.694505    0.672527    0.702128\n",
       "roc_auc      0.726456    0.707332    0.733192"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = run_all_models(X_train.values, y_train.values, X_test.values, y_test.values, epochs=70, batch_size=64)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0fa702e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Network_v1 ===\n",
      "Epoch 1/200 - Loss: 0.6774\n",
      "Epoch 2/200 - Loss: 0.6084\n",
      "Epoch 3/200 - Loss: 0.5602\n",
      "Epoch 4/200 - Loss: 0.5384\n",
      "Epoch 5/200 - Loss: 0.5245\n",
      "Epoch 6/200 - Loss: 0.5199\n",
      "Epoch 7/200 - Loss: 0.5122\n",
      "Epoch 8/200 - Loss: 0.5119\n",
      "Epoch 9/200 - Loss: 0.5070\n",
      "Epoch 10/200 - Loss: 0.5033\n",
      "Epoch 11/200 - Loss: 0.5021\n",
      "Epoch 12/200 - Loss: 0.5033\n",
      "Epoch 13/200 - Loss: 0.4966\n",
      "Epoch 14/200 - Loss: 0.4967\n",
      "Epoch 15/200 - Loss: 0.4882\n",
      "Epoch 16/200 - Loss: 0.4863\n",
      "Epoch 17/200 - Loss: 0.4811\n",
      "Epoch 18/200 - Loss: 0.4802\n",
      "Epoch 19/200 - Loss: 0.4711\n",
      "Epoch 20/200 - Loss: 0.4663\n",
      "Epoch 21/200 - Loss: 0.4645\n",
      "Epoch 22/200 - Loss: 0.4582\n",
      "Epoch 23/200 - Loss: 0.4527\n",
      "Epoch 24/200 - Loss: 0.4505\n",
      "Epoch 25/200 - Loss: 0.4462\n",
      "Epoch 26/200 - Loss: 0.4342\n",
      "Epoch 27/200 - Loss: 0.4350\n",
      "Epoch 28/200 - Loss: 0.4184\n",
      "Epoch 29/200 - Loss: 0.4274\n",
      "Epoch 30/200 - Loss: 0.4175\n",
      "Epoch 31/200 - Loss: 0.4089\n",
      "Epoch 32/200 - Loss: 0.3989\n",
      "Epoch 33/200 - Loss: 0.4040\n",
      "Epoch 34/200 - Loss: 0.3946\n",
      "Epoch 35/200 - Loss: 0.3866\n",
      "Epoch 36/200 - Loss: 0.3900\n",
      "Epoch 37/200 - Loss: 0.3865\n",
      "Epoch 38/200 - Loss: 0.3709\n",
      "Epoch 39/200 - Loss: 0.3737\n",
      "Epoch 40/200 - Loss: 0.3614\n",
      "Epoch 41/200 - Loss: 0.3569\n",
      "Epoch 42/200 - Loss: 0.3500\n",
      "Epoch 43/200 - Loss: 0.3535\n",
      "Epoch 44/200 - Loss: 0.3489\n",
      "Epoch 45/200 - Loss: 0.3377\n",
      "Epoch 46/200 - Loss: 0.3361\n",
      "Epoch 47/200 - Loss: 0.3380\n",
      "Epoch 48/200 - Loss: 0.3222\n",
      "Epoch 49/200 - Loss: 0.3239\n",
      "Epoch 50/200 - Loss: 0.3218\n",
      "Epoch 51/200 - Loss: 0.3115\n",
      "Epoch 52/200 - Loss: 0.3111\n",
      "Epoch 53/200 - Loss: 0.3103\n",
      "Epoch 54/200 - Loss: 0.2990\n",
      "Epoch 55/200 - Loss: 0.3006\n",
      "Epoch 56/200 - Loss: 0.2984\n",
      "Epoch 57/200 - Loss: 0.2869\n",
      "Epoch 58/200 - Loss: 0.2931\n",
      "Epoch 59/200 - Loss: 0.2828\n",
      "Epoch 60/200 - Loss: 0.2728\n",
      "Epoch 61/200 - Loss: 0.2705\n",
      "Epoch 62/200 - Loss: 0.2691\n",
      "Epoch 63/200 - Loss: 0.2579\n",
      "Epoch 64/200 - Loss: 0.2568\n",
      "Epoch 65/200 - Loss: 0.2605\n",
      "Epoch 66/200 - Loss: 0.2601\n",
      "Epoch 67/200 - Loss: 0.2474\n",
      "Epoch 68/200 - Loss: 0.2443\n",
      "Epoch 69/200 - Loss: 0.2428\n",
      "Epoch 70/200 - Loss: 0.2502\n",
      "Epoch 71/200 - Loss: 0.2364\n",
      "Epoch 72/200 - Loss: 0.2276\n",
      "Epoch 73/200 - Loss: 0.2285\n",
      "Epoch 74/200 - Loss: 0.2229\n",
      "Epoch 75/200 - Loss: 0.2258\n",
      "Epoch 76/200 - Loss: 0.2187\n",
      "Epoch 77/200 - Loss: 0.2043\n",
      "Epoch 78/200 - Loss: 0.2117\n",
      "Epoch 79/200 - Loss: 0.2079\n",
      "Epoch 80/200 - Loss: 0.2013\n",
      "Epoch 81/200 - Loss: 0.1916\n",
      "Epoch 82/200 - Loss: 0.1949\n",
      "Epoch 83/200 - Loss: 0.1846\n",
      "Epoch 84/200 - Loss: 0.2032\n",
      "Epoch 85/200 - Loss: 0.1950\n",
      "Epoch 86/200 - Loss: 0.1820\n",
      "Epoch 87/200 - Loss: 0.1855\n",
      "Epoch 88/200 - Loss: 0.1892\n",
      "Epoch 89/200 - Loss: 0.1886\n",
      "Epoch 90/200 - Loss: 0.1809\n",
      "Epoch 91/200 - Loss: 0.1663\n",
      "Epoch 92/200 - Loss: 0.1663\n",
      "Epoch 93/200 - Loss: 0.1746\n",
      "Epoch 94/200 - Loss: 0.1645\n",
      "Epoch 95/200 - Loss: 0.1585\n",
      "Epoch 96/200 - Loss: 0.1545\n",
      "Epoch 97/200 - Loss: 0.1588\n",
      "Epoch 98/200 - Loss: 0.1536\n",
      "Epoch 99/200 - Loss: 0.1410\n",
      "Epoch 100/200 - Loss: 0.1551\n",
      "Epoch 101/200 - Loss: 0.1475\n",
      "Epoch 102/200 - Loss: 0.1394\n",
      "Epoch 103/200 - Loss: 0.1369\n",
      "Epoch 104/200 - Loss: 0.1424\n",
      "Epoch 105/200 - Loss: 0.1384\n",
      "Epoch 106/200 - Loss: 0.1235\n",
      "Epoch 107/200 - Loss: 0.1402\n",
      "Epoch 108/200 - Loss: 0.1363\n",
      "Epoch 109/200 - Loss: 0.1231\n",
      "Epoch 110/200 - Loss: 0.1354\n",
      "Epoch 111/200 - Loss: 0.1258\n",
      "Epoch 112/200 - Loss: 0.1220\n",
      "Epoch 113/200 - Loss: 0.1262\n",
      "Epoch 114/200 - Loss: 0.1262\n",
      "Epoch 115/200 - Loss: 0.1128\n",
      "Epoch 116/200 - Loss: 0.1176\n",
      "Epoch 117/200 - Loss: 0.1175\n",
      "Epoch 118/200 - Loss: 0.1232\n",
      "Epoch 119/200 - Loss: 0.1132\n",
      "Epoch 120/200 - Loss: 0.1120\n",
      "Epoch 121/200 - Loss: 0.1093\n",
      "Epoch 122/200 - Loss: 0.1157\n",
      "Epoch 123/200 - Loss: 0.1032\n",
      "Epoch 124/200 - Loss: 0.0960\n",
      "Epoch 125/200 - Loss: 0.1066\n",
      "Epoch 126/200 - Loss: 0.0987\n",
      "Epoch 127/200 - Loss: 0.1170\n",
      "Epoch 128/200 - Loss: 0.1001\n",
      "Epoch 129/200 - Loss: 0.0858\n",
      "Epoch 130/200 - Loss: 0.0920\n",
      "Epoch 131/200 - Loss: 0.0914\n",
      "Epoch 132/200 - Loss: 0.0980\n",
      "Epoch 133/200 - Loss: 0.0920\n",
      "Epoch 134/200 - Loss: 0.0831\n",
      "Epoch 135/200 - Loss: 0.1011\n",
      "Epoch 136/200 - Loss: 0.1014\n",
      "Epoch 137/200 - Loss: 0.0968\n",
      "Epoch 138/200 - Loss: 0.0866\n",
      "Epoch 139/200 - Loss: 0.0924\n",
      "Epoch 140/200 - Loss: 0.0930\n",
      "Epoch 141/200 - Loss: 0.0833\n",
      "Epoch 142/200 - Loss: 0.0822\n",
      "Epoch 143/200 - Loss: 0.0918\n",
      "Epoch 144/200 - Loss: 0.0854\n",
      "Epoch 145/200 - Loss: 0.0855\n",
      "Epoch 146/200 - Loss: 0.0915\n",
      "Epoch 147/200 - Loss: 0.0763\n",
      "Epoch 148/200 - Loss: 0.0688\n",
      "Epoch 149/200 - Loss: 0.0764\n",
      "Epoch 150/200 - Loss: 0.0674\n",
      "Epoch 151/200 - Loss: 0.0781\n",
      "Epoch 152/200 - Loss: 0.0753\n",
      "Epoch 153/200 - Loss: 0.0728\n",
      "Epoch 154/200 - Loss: 0.0604\n",
      "Epoch 155/200 - Loss: 0.0779\n",
      "Epoch 156/200 - Loss: 0.0811\n",
      "Epoch 157/200 - Loss: 0.0626\n",
      "Epoch 158/200 - Loss: 0.0686\n",
      "Epoch 159/200 - Loss: 0.0817\n",
      "Epoch 160/200 - Loss: 0.0628\n",
      "Epoch 161/200 - Loss: 0.0671\n",
      "Epoch 162/200 - Loss: 0.0569\n",
      "Epoch 163/200 - Loss: 0.0655\n",
      "Epoch 164/200 - Loss: 0.0628\n",
      "Epoch 165/200 - Loss: 0.0580\n",
      "Epoch 166/200 - Loss: 0.0692\n",
      "Epoch 167/200 - Loss: 0.0608\n",
      "Epoch 168/200 - Loss: 0.0500\n",
      "Epoch 169/200 - Loss: 0.0669\n",
      "Epoch 170/200 - Loss: 0.0744\n",
      "Epoch 171/200 - Loss: 0.0571\n",
      "Epoch 172/200 - Loss: 0.0523\n",
      "Epoch 173/200 - Loss: 0.0642\n",
      "Epoch 174/200 - Loss: 0.0609\n",
      "Epoch 175/200 - Loss: 0.0622\n",
      "Epoch 176/200 - Loss: 0.0461\n",
      "Epoch 177/200 - Loss: 0.0490\n",
      "Epoch 178/200 - Loss: 0.0754\n",
      "Epoch 179/200 - Loss: 0.0617\n",
      "Epoch 180/200 - Loss: 0.0468\n",
      "Epoch 181/200 - Loss: 0.0605\n",
      "Epoch 182/200 - Loss: 0.0620\n",
      "Epoch 183/200 - Loss: 0.0591\n",
      "Epoch 184/200 - Loss: 0.0451\n",
      "Epoch 185/200 - Loss: 0.0671\n",
      "Epoch 186/200 - Loss: 0.0538\n",
      "Epoch 187/200 - Loss: 0.0488\n",
      "Epoch 188/200 - Loss: 0.0403\n",
      "Epoch 189/200 - Loss: 0.0497\n",
      "Epoch 190/200 - Loss: 0.0518\n",
      "Epoch 191/200 - Loss: 0.0544\n",
      "Epoch 192/200 - Loss: 0.0511\n",
      "Epoch 193/200 - Loss: 0.0475\n",
      "Epoch 194/200 - Loss: 0.0465\n",
      "Epoch 195/200 - Loss: 0.0546\n",
      "Epoch 196/200 - Loss: 0.0498\n",
      "Epoch 197/200 - Loss: 0.0522\n",
      "Epoch 198/200 - Loss: 0.0533\n",
      "Epoch 199/200 - Loss: 0.0591\n",
      "Epoch 200/200 - Loss: 0.0517\n",
      "\n",
      "Evaluation Results\n",
      "Accuracy: 0.6484\n",
      "Precision: 0.6270\n",
      "Recall: 0.7083\n",
      "F1 Score: 0.6652\n",
      "ROC AUC: 0.6961\n",
      "------------------------------\n",
      "\n",
      "=== Training Network_v2 ===\n",
      "Epoch 1/200 - Loss: 0.6865\n",
      "Epoch 2/200 - Loss: 0.6286\n",
      "Epoch 3/200 - Loss: 0.5573\n",
      "Epoch 4/200 - Loss: 0.5280\n",
      "Epoch 5/200 - Loss: 0.5299\n",
      "Epoch 6/200 - Loss: 0.5246\n",
      "Epoch 7/200 - Loss: 0.5112\n",
      "Epoch 8/200 - Loss: 0.5133\n",
      "Epoch 9/200 - Loss: 0.5032\n",
      "Epoch 10/200 - Loss: 0.5056\n",
      "Epoch 11/200 - Loss: 0.4955\n",
      "Epoch 12/200 - Loss: 0.4952\n",
      "Epoch 13/200 - Loss: 0.4926\n",
      "Epoch 14/200 - Loss: 0.4887\n",
      "Epoch 15/200 - Loss: 0.4815\n",
      "Epoch 16/200 - Loss: 0.4744\n",
      "Epoch 17/200 - Loss: 0.4666\n",
      "Epoch 18/200 - Loss: 0.4633\n",
      "Epoch 19/200 - Loss: 0.4596\n",
      "Epoch 20/200 - Loss: 0.4515\n",
      "Epoch 21/200 - Loss: 0.4469\n",
      "Epoch 22/200 - Loss: 0.4395\n",
      "Epoch 23/200 - Loss: 0.4332\n",
      "Epoch 24/200 - Loss: 0.4298\n",
      "Epoch 25/200 - Loss: 0.4237\n",
      "Epoch 26/200 - Loss: 0.4220\n",
      "Epoch 27/200 - Loss: 0.4213\n",
      "Epoch 28/200 - Loss: 0.4020\n",
      "Epoch 29/200 - Loss: 0.4074\n",
      "Epoch 30/200 - Loss: 0.4047\n",
      "Epoch 31/200 - Loss: 0.3949\n",
      "Epoch 32/200 - Loss: 0.3868\n",
      "Epoch 33/200 - Loss: 0.3852\n",
      "Epoch 34/200 - Loss: 0.3773\n",
      "Epoch 35/200 - Loss: 0.3673\n",
      "Epoch 36/200 - Loss: 0.3650\n",
      "Epoch 37/200 - Loss: 0.3592\n",
      "Epoch 38/200 - Loss: 0.3639\n",
      "Epoch 39/200 - Loss: 0.3594\n",
      "Epoch 40/200 - Loss: 0.3490\n",
      "Epoch 41/200 - Loss: 0.3441\n",
      "Epoch 42/200 - Loss: 0.3448\n",
      "Epoch 43/200 - Loss: 0.3393\n",
      "Epoch 44/200 - Loss: 0.3268\n",
      "Epoch 45/200 - Loss: 0.3200\n",
      "Epoch 46/200 - Loss: 0.3229\n",
      "Epoch 47/200 - Loss: 0.3153\n",
      "Epoch 48/200 - Loss: 0.2997\n",
      "Epoch 49/200 - Loss: 0.2983\n",
      "Epoch 50/200 - Loss: 0.3013\n",
      "Epoch 51/200 - Loss: 0.2914\n",
      "Epoch 52/200 - Loss: 0.2899\n",
      "Epoch 53/200 - Loss: 0.2829\n",
      "Epoch 54/200 - Loss: 0.2822\n",
      "Epoch 55/200 - Loss: 0.2681\n",
      "Epoch 56/200 - Loss: 0.2757\n",
      "Epoch 57/200 - Loss: 0.2712\n",
      "Epoch 58/200 - Loss: 0.2639\n",
      "Epoch 59/200 - Loss: 0.2733\n",
      "Epoch 60/200 - Loss: 0.2570\n",
      "Epoch 61/200 - Loss: 0.2371\n",
      "Epoch 62/200 - Loss: 0.2476\n",
      "Epoch 63/200 - Loss: 0.2526\n",
      "Epoch 64/200 - Loss: 0.2377\n",
      "Epoch 65/200 - Loss: 0.2333\n",
      "Epoch 66/200 - Loss: 0.2438\n",
      "Epoch 67/200 - Loss: 0.2262\n",
      "Epoch 68/200 - Loss: 0.2340\n",
      "Epoch 69/200 - Loss: 0.2230\n",
      "Epoch 70/200 - Loss: 0.2328\n",
      "Epoch 71/200 - Loss: 0.2240\n",
      "Epoch 72/200 - Loss: 0.2163\n",
      "Epoch 73/200 - Loss: 0.2101\n",
      "Epoch 74/200 - Loss: 0.2067\n",
      "Epoch 75/200 - Loss: 0.2097\n",
      "Epoch 76/200 - Loss: 0.2100\n",
      "Epoch 77/200 - Loss: 0.1990\n",
      "Epoch 78/200 - Loss: 0.2020\n",
      "Epoch 79/200 - Loss: 0.2039\n",
      "Epoch 80/200 - Loss: 0.1909\n",
      "Epoch 81/200 - Loss: 0.1944\n",
      "Epoch 82/200 - Loss: 0.1891\n",
      "Epoch 83/200 - Loss: 0.1943\n",
      "Epoch 84/200 - Loss: 0.1903\n",
      "Epoch 85/200 - Loss: 0.1806\n",
      "Epoch 86/200 - Loss: 0.1743\n",
      "Epoch 87/200 - Loss: 0.1906\n",
      "Epoch 88/200 - Loss: 0.1883\n",
      "Epoch 89/200 - Loss: 0.1639\n",
      "Epoch 90/200 - Loss: 0.1696\n",
      "Epoch 91/200 - Loss: 0.1713\n",
      "Epoch 92/200 - Loss: 0.1749\n",
      "Epoch 93/200 - Loss: 0.1630\n",
      "Epoch 94/200 - Loss: 0.1713\n",
      "Epoch 95/200 - Loss: 0.1722\n",
      "Epoch 96/200 - Loss: 0.1605\n",
      "Epoch 97/200 - Loss: 0.1505\n",
      "Epoch 98/200 - Loss: 0.1534\n",
      "Epoch 99/200 - Loss: 0.1445\n",
      "Epoch 100/200 - Loss: 0.1405\n",
      "Epoch 101/200 - Loss: 0.1593\n",
      "Epoch 102/200 - Loss: 0.1514\n",
      "Epoch 103/200 - Loss: 0.1356\n",
      "Epoch 104/200 - Loss: 0.1433\n",
      "Epoch 105/200 - Loss: 0.1322\n",
      "Epoch 106/200 - Loss: 0.1385\n",
      "Epoch 107/200 - Loss: 0.1392\n",
      "Epoch 108/200 - Loss: 0.1205\n",
      "Epoch 109/200 - Loss: 0.1304\n",
      "Epoch 110/200 - Loss: 0.1330\n",
      "Epoch 111/200 - Loss: 0.1460\n",
      "Epoch 112/200 - Loss: 0.1250\n",
      "Epoch 113/200 - Loss: 0.1214\n",
      "Epoch 114/200 - Loss: 0.1219\n",
      "Epoch 115/200 - Loss: 0.1213\n",
      "Epoch 116/200 - Loss: 0.1312\n",
      "Epoch 117/200 - Loss: 0.1337\n",
      "Epoch 118/200 - Loss: 0.1012\n",
      "Epoch 119/200 - Loss: 0.1321\n",
      "Epoch 120/200 - Loss: 0.1259\n",
      "Epoch 121/200 - Loss: 0.1203\n",
      "Epoch 122/200 - Loss: 0.1130\n",
      "Epoch 123/200 - Loss: 0.0971\n",
      "Epoch 124/200 - Loss: 0.1067\n",
      "Epoch 125/200 - Loss: 0.0983\n",
      "Epoch 126/200 - Loss: 0.1075\n",
      "Epoch 127/200 - Loss: 0.1287\n",
      "Epoch 128/200 - Loss: 0.0987\n",
      "Epoch 129/200 - Loss: 0.1101\n",
      "Epoch 130/200 - Loss: 0.0929\n",
      "Epoch 131/200 - Loss: 0.1095\n",
      "Epoch 132/200 - Loss: 0.1105\n",
      "Epoch 133/200 - Loss: 0.1007\n",
      "Epoch 134/200 - Loss: 0.0928\n",
      "Epoch 135/200 - Loss: 0.0853\n",
      "Epoch 136/200 - Loss: 0.0911\n",
      "Epoch 137/200 - Loss: 0.1057\n",
      "Epoch 138/200 - Loss: 0.0884\n",
      "Epoch 139/200 - Loss: 0.0943\n",
      "Epoch 140/200 - Loss: 0.0749\n",
      "Epoch 141/200 - Loss: 0.0748\n",
      "Epoch 142/200 - Loss: 0.0797\n",
      "Epoch 143/200 - Loss: 0.0777\n",
      "Epoch 144/200 - Loss: 0.0788\n",
      "Epoch 145/200 - Loss: 0.0667\n",
      "Epoch 146/200 - Loss: 0.0766\n",
      "Epoch 147/200 - Loss: 0.0719\n",
      "Epoch 148/200 - Loss: 0.1010\n",
      "Epoch 149/200 - Loss: 0.0870\n",
      "Epoch 150/200 - Loss: 0.0713\n",
      "Epoch 151/200 - Loss: 0.0691\n",
      "Epoch 152/200 - Loss: 0.0750\n",
      "Epoch 153/200 - Loss: 0.0766\n",
      "Epoch 154/200 - Loss: 0.0720\n",
      "Epoch 155/200 - Loss: 0.0731\n",
      "Epoch 156/200 - Loss: 0.0777\n",
      "Epoch 157/200 - Loss: 0.0788\n",
      "Epoch 158/200 - Loss: 0.0772\n",
      "Epoch 159/200 - Loss: 0.0729\n",
      "Epoch 160/200 - Loss: 0.0620\n",
      "Epoch 161/200 - Loss: 0.0597\n",
      "Epoch 162/200 - Loss: 0.0593\n",
      "Epoch 163/200 - Loss: 0.0899\n",
      "Epoch 164/200 - Loss: 0.0746\n",
      "Epoch 165/200 - Loss: 0.0651\n",
      "Epoch 166/200 - Loss: 0.0651\n",
      "Epoch 167/200 - Loss: 0.0603\n",
      "Epoch 168/200 - Loss: 0.0784\n",
      "Epoch 169/200 - Loss: 0.0653\n",
      "Epoch 170/200 - Loss: 0.0560\n",
      "Epoch 171/200 - Loss: 0.0796\n",
      "Epoch 172/200 - Loss: 0.0672\n",
      "Epoch 173/200 - Loss: 0.0500\n",
      "Epoch 174/200 - Loss: 0.0835\n",
      "Epoch 175/200 - Loss: 0.0670\n",
      "Epoch 176/200 - Loss: 0.0684\n",
      "Epoch 177/200 - Loss: 0.0611\n",
      "Epoch 178/200 - Loss: 0.0602\n",
      "Epoch 179/200 - Loss: 0.0580\n",
      "Epoch 180/200 - Loss: 0.0522\n",
      "Epoch 181/200 - Loss: 0.0565\n",
      "Epoch 182/200 - Loss: 0.0522\n",
      "Epoch 183/200 - Loss: 0.0573\n",
      "Epoch 184/200 - Loss: 0.0567\n",
      "Epoch 185/200 - Loss: 0.0633\n",
      "Epoch 186/200 - Loss: 0.0609\n",
      "Epoch 187/200 - Loss: 0.0695\n",
      "Epoch 188/200 - Loss: 0.0691\n",
      "Epoch 189/200 - Loss: 0.0463\n",
      "Epoch 190/200 - Loss: 0.0486\n",
      "Epoch 191/200 - Loss: 0.0478\n",
      "Epoch 192/200 - Loss: 0.0530\n",
      "Epoch 193/200 - Loss: 0.0614\n",
      "Epoch 194/200 - Loss: 0.0546\n",
      "Epoch 195/200 - Loss: 0.0647\n",
      "Epoch 196/200 - Loss: 0.0508\n",
      "Epoch 197/200 - Loss: 0.0454\n",
      "Epoch 198/200 - Loss: 0.0535\n",
      "Epoch 199/200 - Loss: 0.0624\n",
      "Epoch 200/200 - Loss: 0.0608\n",
      "\n",
      "Evaluation Results\n",
      "Accuracy: 0.6507\n",
      "Precision: 0.6226\n",
      "Recall: 0.7407\n",
      "F1 Score: 0.6765\n",
      "ROC AUC: 0.7046\n",
      "------------------------------\n",
      "\n",
      "=== Training Network_v3 ===\n",
      "Epoch 1/200 - Loss: 0.6828\n",
      "Epoch 2/200 - Loss: 0.6135\n",
      "Epoch 3/200 - Loss: 0.5459\n",
      "Epoch 4/200 - Loss: 0.5287\n",
      "Epoch 5/200 - Loss: 0.5205\n",
      "Epoch 6/200 - Loss: 0.5166\n",
      "Epoch 7/200 - Loss: 0.5139\n",
      "Epoch 8/200 - Loss: 0.5084\n",
      "Epoch 9/200 - Loss: 0.5068\n",
      "Epoch 10/200 - Loss: 0.5023\n",
      "Epoch 11/200 - Loss: 0.4938\n",
      "Epoch 12/200 - Loss: 0.4913\n",
      "Epoch 13/200 - Loss: 0.4892\n",
      "Epoch 14/200 - Loss: 0.4804\n",
      "Epoch 15/200 - Loss: 0.4752\n",
      "Epoch 16/200 - Loss: 0.4738\n",
      "Epoch 17/200 - Loss: 0.4712\n",
      "Epoch 18/200 - Loss: 0.4583\n",
      "Epoch 19/200 - Loss: 0.4607\n",
      "Epoch 20/200 - Loss: 0.4417\n",
      "Epoch 21/200 - Loss: 0.4384\n",
      "Epoch 22/200 - Loss: 0.4326\n",
      "Epoch 23/200 - Loss: 0.4215\n",
      "Epoch 24/200 - Loss: 0.4285\n",
      "Epoch 25/200 - Loss: 0.4136\n",
      "Epoch 26/200 - Loss: 0.4127\n",
      "Epoch 27/200 - Loss: 0.4014\n",
      "Epoch 28/200 - Loss: 0.4060\n",
      "Epoch 29/200 - Loss: 0.3913\n",
      "Epoch 30/200 - Loss: 0.3727\n",
      "Epoch 31/200 - Loss: 0.3846\n",
      "Epoch 32/200 - Loss: 0.3710\n",
      "Epoch 33/200 - Loss: 0.3639\n",
      "Epoch 34/200 - Loss: 0.3488\n",
      "Epoch 35/200 - Loss: 0.3445\n",
      "Epoch 36/200 - Loss: 0.3488\n",
      "Epoch 37/200 - Loss: 0.3330\n",
      "Epoch 38/200 - Loss: 0.3362\n",
      "Epoch 39/200 - Loss: 0.3325\n",
      "Epoch 40/200 - Loss: 0.3196\n",
      "Epoch 41/200 - Loss: 0.3169\n",
      "Epoch 42/200 - Loss: 0.3059\n",
      "Epoch 43/200 - Loss: 0.3039\n",
      "Epoch 44/200 - Loss: 0.3194\n",
      "Epoch 45/200 - Loss: 0.3039\n",
      "Epoch 46/200 - Loss: 0.2903\n",
      "Epoch 47/200 - Loss: 0.2862\n",
      "Epoch 48/200 - Loss: 0.2937\n",
      "Epoch 49/200 - Loss: 0.2681\n",
      "Epoch 50/200 - Loss: 0.2627\n",
      "Epoch 51/200 - Loss: 0.2656\n",
      "Epoch 52/200 - Loss: 0.2549\n",
      "Epoch 53/200 - Loss: 0.2527\n",
      "Epoch 54/200 - Loss: 0.2427\n",
      "Epoch 55/200 - Loss: 0.2410\n",
      "Epoch 56/200 - Loss: 0.2568\n",
      "Epoch 57/200 - Loss: 0.2483\n",
      "Epoch 58/200 - Loss: 0.2350\n",
      "Epoch 59/200 - Loss: 0.2327\n",
      "Epoch 60/200 - Loss: 0.2374\n",
      "Epoch 61/200 - Loss: 0.2245\n",
      "Epoch 62/200 - Loss: 0.2270\n",
      "Epoch 63/200 - Loss: 0.1905\n",
      "Epoch 64/200 - Loss: 0.1967\n",
      "Epoch 65/200 - Loss: 0.2015\n",
      "Epoch 66/200 - Loss: 0.2405\n",
      "Epoch 67/200 - Loss: 0.2138\n",
      "Epoch 68/200 - Loss: 0.1826\n",
      "Epoch 69/200 - Loss: 0.1923\n",
      "Epoch 70/200 - Loss: 0.1933\n",
      "Epoch 71/200 - Loss: 0.1871\n",
      "Epoch 72/200 - Loss: 0.1691\n",
      "Epoch 73/200 - Loss: 0.1743\n",
      "Epoch 74/200 - Loss: 0.1846\n",
      "Epoch 75/200 - Loss: 0.1603\n",
      "Epoch 76/200 - Loss: 0.1705\n",
      "Epoch 77/200 - Loss: 0.1590\n",
      "Epoch 78/200 - Loss: 0.1613\n",
      "Epoch 79/200 - Loss: 0.1757\n",
      "Epoch 80/200 - Loss: 0.1620\n",
      "Epoch 81/200 - Loss: 0.1669\n",
      "Epoch 82/200 - Loss: 0.1544\n",
      "Epoch 83/200 - Loss: 0.1403\n",
      "Epoch 84/200 - Loss: 0.1802\n",
      "Epoch 85/200 - Loss: 0.1431\n",
      "Epoch 86/200 - Loss: 0.1440\n",
      "Epoch 87/200 - Loss: 0.1520\n",
      "Epoch 88/200 - Loss: 0.1328\n",
      "Epoch 89/200 - Loss: 0.1302\n",
      "Epoch 90/200 - Loss: 0.1222\n",
      "Epoch 91/200 - Loss: 0.1253\n",
      "Epoch 92/200 - Loss: 0.1232\n",
      "Epoch 93/200 - Loss: 0.1493\n",
      "Epoch 94/200 - Loss: 0.1178\n",
      "Epoch 95/200 - Loss: 0.1299\n",
      "Epoch 96/200 - Loss: 0.1070\n",
      "Epoch 97/200 - Loss: 0.1186\n",
      "Epoch 98/200 - Loss: 0.1046\n",
      "Epoch 99/200 - Loss: 0.1139\n",
      "Epoch 100/200 - Loss: 0.1019\n",
      "Epoch 101/200 - Loss: 0.1197\n",
      "Epoch 102/200 - Loss: 0.1116\n",
      "Epoch 103/200 - Loss: 0.1078\n",
      "Epoch 104/200 - Loss: 0.0953\n",
      "Epoch 105/200 - Loss: 0.0929\n",
      "Epoch 106/200 - Loss: 0.1180\n",
      "Epoch 107/200 - Loss: 0.1086\n",
      "Epoch 108/200 - Loss: 0.0847\n",
      "Epoch 109/200 - Loss: 0.0908\n",
      "Epoch 110/200 - Loss: 0.1036\n",
      "Epoch 111/200 - Loss: 0.0803\n",
      "Epoch 112/200 - Loss: 0.0847\n",
      "Epoch 113/200 - Loss: 0.0965\n",
      "Epoch 114/200 - Loss: 0.0990\n",
      "Epoch 115/200 - Loss: 0.0695\n",
      "Epoch 116/200 - Loss: 0.0773\n",
      "Epoch 117/200 - Loss: 0.0904\n",
      "Epoch 118/200 - Loss: 0.0891\n",
      "Epoch 119/200 - Loss: 0.0890\n",
      "Epoch 120/200 - Loss: 0.0706\n",
      "Epoch 121/200 - Loss: 0.0732\n",
      "Epoch 122/200 - Loss: 0.0598\n",
      "Epoch 123/200 - Loss: 0.0737\n",
      "Epoch 124/200 - Loss: 0.0803\n",
      "Epoch 125/200 - Loss: 0.0776\n",
      "Epoch 126/200 - Loss: 0.0741\n",
      "Epoch 127/200 - Loss: 0.0573\n",
      "Epoch 128/200 - Loss: 0.0775\n",
      "Epoch 129/200 - Loss: 0.0660\n",
      "Epoch 130/200 - Loss: 0.0584\n",
      "Epoch 131/200 - Loss: 0.0630\n",
      "Epoch 132/200 - Loss: 0.0656\n",
      "Epoch 133/200 - Loss: 0.0514\n",
      "Epoch 134/200 - Loss: 0.0685\n",
      "Epoch 135/200 - Loss: 0.0691\n",
      "Epoch 136/200 - Loss: 0.0649\n",
      "Epoch 137/200 - Loss: 0.0694\n",
      "Epoch 138/200 - Loss: 0.0621\n",
      "Epoch 139/200 - Loss: 0.0528\n",
      "Epoch 140/200 - Loss: 0.0534\n",
      "Epoch 141/200 - Loss: 0.0685\n",
      "Epoch 142/200 - Loss: 0.0734\n",
      "Epoch 143/200 - Loss: 0.0704\n",
      "Epoch 144/200 - Loss: 0.0428\n",
      "Epoch 145/200 - Loss: 0.0667\n",
      "Epoch 146/200 - Loss: 0.0496\n",
      "Epoch 147/200 - Loss: 0.0628\n",
      "Epoch 148/200 - Loss: 0.0459\n",
      "Epoch 149/200 - Loss: 0.0594\n",
      "Epoch 150/200 - Loss: 0.0599\n",
      "Epoch 151/200 - Loss: 0.0504\n",
      "Epoch 152/200 - Loss: 0.0494\n",
      "Epoch 153/200 - Loss: 0.0490\n",
      "Epoch 154/200 - Loss: 0.0599\n",
      "Epoch 155/200 - Loss: 0.0524\n",
      "Epoch 156/200 - Loss: 0.0424\n",
      "Epoch 157/200 - Loss: 0.0520\n",
      "Epoch 158/200 - Loss: 0.0598\n",
      "Epoch 159/200 - Loss: 0.0680\n",
      "Epoch 160/200 - Loss: 0.0430\n",
      "Epoch 161/200 - Loss: 0.0439\n",
      "Epoch 162/200 - Loss: 0.0382\n",
      "Epoch 163/200 - Loss: 0.0498\n",
      "Epoch 164/200 - Loss: 0.0455\n",
      "Epoch 165/200 - Loss: 0.0410\n",
      "Epoch 166/200 - Loss: 0.0361\n",
      "Epoch 167/200 - Loss: 0.0491\n",
      "Epoch 168/200 - Loss: 0.0509\n",
      "Epoch 169/200 - Loss: 0.0444\n",
      "Epoch 170/200 - Loss: 0.0380\n",
      "Epoch 171/200 - Loss: 0.0356\n",
      "Epoch 172/200 - Loss: 0.0483\n",
      "Epoch 173/200 - Loss: 0.0441\n",
      "Epoch 174/200 - Loss: 0.0465\n",
      "Epoch 175/200 - Loss: 0.0503\n",
      "Epoch 176/200 - Loss: 0.0429\n",
      "Epoch 177/200 - Loss: 0.0367\n",
      "Epoch 178/200 - Loss: 0.0363\n",
      "Epoch 179/200 - Loss: 0.0309\n",
      "Epoch 180/200 - Loss: 0.0365\n",
      "Epoch 181/200 - Loss: 0.0307\n",
      "Epoch 182/200 - Loss: 0.0399\n",
      "Epoch 183/200 - Loss: 0.0333\n",
      "Epoch 184/200 - Loss: 0.0308\n",
      "Epoch 185/200 - Loss: 0.0354\n",
      "Epoch 186/200 - Loss: 0.0440\n",
      "Epoch 187/200 - Loss: 0.0361\n",
      "Epoch 188/200 - Loss: 0.0298\n",
      "Epoch 189/200 - Loss: 0.0305\n",
      "Epoch 190/200 - Loss: 0.0426\n",
      "Epoch 191/200 - Loss: 0.0277\n",
      "Epoch 192/200 - Loss: 0.0403\n",
      "Epoch 193/200 - Loss: 0.0409\n",
      "Epoch 194/200 - Loss: 0.0422\n",
      "Epoch 195/200 - Loss: 0.0286\n",
      "Epoch 196/200 - Loss: 0.0273\n",
      "Epoch 197/200 - Loss: 0.0251\n",
      "Epoch 198/200 - Loss: 0.0251\n",
      "Epoch 199/200 - Loss: 0.0240\n",
      "Epoch 200/200 - Loss: 0.0310\n",
      "\n",
      "Evaluation Results\n",
      "Accuracy: 0.6621\n",
      "Precision: 0.6504\n",
      "Recall: 0.6806\n",
      "F1 Score: 0.6652\n",
      "ROC AUC: 0.7123\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Network_v1</th>\n",
       "      <th>Network_v2</th>\n",
       "      <th>Network_v3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.648402</td>\n",
       "      <td>0.650685</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.627049</td>\n",
       "      <td>0.622568</td>\n",
       "      <td>0.650442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.665217</td>\n",
       "      <td>0.676533</td>\n",
       "      <td>0.665158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.696071</td>\n",
       "      <td>0.704569</td>\n",
       "      <td>0.712264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Network_v1  Network_v2  Network_v3\n",
       "accuracy     0.648402    0.650685    0.662100\n",
       "precision    0.627049    0.622568    0.650442\n",
       "recall       0.708333    0.740741    0.680556\n",
       "f1_score     0.665217    0.676533    0.665158\n",
       "roc_auc      0.696071    0.704569    0.712264"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = run_all_models(X_train.values, y_train.values, X_test.values, y_test.values, epochs=200, batch_size=64)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48567293",
   "metadata": {},
   "source": [
    "**Conclusion:** Even the neural network was not able to outperform the tree-based ensembles in evaluation metrics. Thus, the best accuracy achieved by any of our model is **75%**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring_Financial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
