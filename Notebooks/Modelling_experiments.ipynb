{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02691b67",
   "metadata": {},
   "source": [
    "## Modelling experiments guidelines:\n",
    "\n",
    "1. The problem provided to us is a `Binary Classification problem`. The raw dataset is already pre-processed and saved. We will work with that processed dataset here for training different models and performing inference.\n",
    "\n",
    "2. Multiple models will be tried out here, ranging from baselines such as Logistic Regressions, KNN, SVM to tree-based ensembles like XgBoost, Random Forest, CatBoost etc to Neural Networks and Probabilistic models like Gaussian Processes (GP).\n",
    "\n",
    "3. Once done, we will also try to perform clustering of the dataset after doing PCA/t-SNE (to convert to 2D) inorder to get an idea about the spread of different classes in 2D.\n",
    "\n",
    "4. One important thing to note is that the problem statement asked to train the model on entire dataset and save it. However, for our purpose, due to the lack of test/unseen data, we will split our available data into training and test set, train models only on the training set and evaluate it on the unseen data.\n",
    "\n",
    "5. In this data, we will choose and report the best model. However the final models that will be saved in the `Models/` folder will be trained on the `entire dataset`.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996932fb",
   "metadata": {},
   "source": [
    "### Benchmarks:-\n",
    "\n",
    "|S No. |  Model Name  |    Test accuracy  | Test Precision |  Test Recall   |   Test F1   |\n",
    "|------|--------------|--------------------|---------------|----------------|-------------|\n",
    "|  1.  |  Logistic Regression | 1          |  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcd7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91629\\Desktop\\Spring_Financial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2188, 101) (2188, 101) (2188, 103) (2188, 103)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "#   Load the Data path\n",
    "env_path = Path(os.getcwd()).parent / 'Config' / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "PREPROCESSED_PATH = os.getenv('PREPROCESSED_PATH')\n",
    "\n",
    "\n",
    "#   Load all the 4 datasets. They will be used for training different models as per convenience\n",
    "unprocessed_df = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'unprocessed_data.csv'))\n",
    "\n",
    "final_df_qt = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df.csv'))\n",
    "final_df_not_qt = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df_not_qt.csv'))\n",
    "\n",
    "qt_df_with_corr = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df_with_corr.csv'))\n",
    "not_qt_with_corr = pd.read_csv(os.path.join(PREPROCESSED_PATH, 'final_df_not_qt_with_corr.csv'))\n",
    "\n",
    "\n",
    "print(final_df_qt.shape, final_df_not_qt.shape, qt_df_with_corr.shape, not_qt_with_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ddcb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    1117\n",
      "0    1071\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#   The target variable is well-balanced. No need for any additional operations\n",
    "print(final_df_qt['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc38e9",
   "metadata": {},
   "source": [
    "To begin with, we will start with Baseline Models like Logistic Regression etc. For these models, the quantiled datasets like `final_df_qt` and `qt_df_with_corr` suits the best. We will do all our experiments with the `final_df_qt` and later repeat with the latter, keeping the same code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ad87e",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1daa7ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 100) (438, 100) (1750,) (438,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = final_df_qt.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.20, shuffle=True, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889745de",
   "metadata": {},
   "source": [
    "### Lets automate the model fitting, training and testing part:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b914cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a827da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_and_evaluate(X_train, X_test, y_train, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred, zero_division=0))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred, zero_division=0))\n",
    "    if auc is not None:\n",
    "        print(\"ROC AUC:\", auc)\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87d64",
   "metadata": {},
   "source": [
    "#### Train and evaluate on some baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff0df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Accuracy: 0.7123287671232876\n",
      "Precision: 0.6991150442477876\n",
      "Recall: 0.7314814814814815\n",
      "F1 Score: 0.7149321266968326\n",
      "ROC AUC: 0.7816358024691359\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.7123287671232876\n",
      "Precision: 0.6939655172413793\n",
      "Recall: 0.7453703703703703\n",
      "F1 Score: 0.71875\n",
      "ROC AUC: 0.7946800967634301\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.7214611872146118\n",
      "Precision: 0.7117117117117117\n",
      "Recall: 0.7314814814814815\n",
      "F1 Score: 0.7214611872146118\n",
      "ROC AUC: 0.7832311478144812\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.6894977168949772\n",
      "Precision: 0.6769911504424779\n",
      "Recall: 0.7083333333333334\n",
      "F1 Score: 0.6923076923076923\n",
      "ROC AUC: 0.7683725392058726\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.682648401826484\n",
      "Precision: 0.672645739910314\n",
      "Recall: 0.6944444444444444\n",
      "F1 Score: 0.683371298405467\n",
      "ROC AUC: 0.7606773440106774\n",
      "------------------------------\n",
      "Model: KNeighborsClassifier\n",
      "Accuracy: 0.6552511415525114\n",
      "Precision: 0.620817843866171\n",
      "Recall: 0.7731481481481481\n",
      "F1 Score: 0.688659793814433\n",
      "ROC AUC: 0.7135364531197865\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#   Seems like SVC(degree=2) and Logisitic regression are the top perfomers (around ~ 71% accuracy)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    SVC(kernel='rbf', probability=True),             \n",
    "    SVC(kernel='poly', degree=2, probability=True),\n",
    "    SVC(kernel='poly', degree=3, probability=True),  \n",
    "    SVC(kernel='poly', degree=4, probability=True),  \n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "# Run evaluation for all models\n",
    "for model in models:\n",
    "    model_fit_and_evaluate(X_train, X_test, y_train, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebba18",
   "metadata": {},
   "source": [
    "#### Now I want to check if our pre-processing even added any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42141695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 48) (438, 48) (1750,) (438,)\n"
     ]
    }
   ],
   "source": [
    "df = unprocessed_df.copy()\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(unprocessed_df.drop('target', axis=1), unprocessed_df['target'], test_size=0.20, shuffle=True, random_state=42)\n",
    "print(X_train_u.shape, X_test_u.shape, y_train_u.shape, y_test_u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91629\\Desktop\\Spring_Financial\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Accuracy: 0.7146118721461188\n",
      "Precision: 0.6842105263157895\n",
      "Recall: 0.7824074074074074\n",
      "F1 Score: 0.7300215982721382\n",
      "ROC AUC: 0.7743576910243577\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.684931506849315\n",
      "Precision: 0.6433823529411765\n",
      "Recall: 0.8101851851851852\n",
      "F1 Score: 0.7172131147540983\n",
      "ROC AUC: 0.7447238905572238\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.6164383561643836\n",
      "Precision: 0.5710059171597633\n",
      "Recall: 0.8935185185185185\n",
      "F1 Score: 0.6967509025270758\n",
      "ROC AUC: 0.6948302469135802\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.591324200913242\n",
      "Precision: 0.5506849315068493\n",
      "Recall: 0.9305555555555556\n",
      "F1 Score: 0.6919104991394148\n",
      "ROC AUC: 0.6536953620286955\n",
      "------------------------------\n",
      "Model: SVC\n",
      "Accuracy: 0.5662100456621004\n",
      "Precision: 0.5347593582887701\n",
      "Recall: 0.9259259259259259\n",
      "F1 Score: 0.6779661016949152\n",
      "ROC AUC: 0.6158658658658659\n",
      "------------------------------\n",
      "Model: KNeighborsClassifier\n",
      "Accuracy: 0.6027397260273972\n",
      "Precision: 0.58203125\n",
      "Recall: 0.6898148148148148\n",
      "F1 Score: 0.6313559322033898\n",
      "ROC AUC: 0.6254796463129797\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    SVC(kernel='rbf', probability=True),             \n",
    "    SVC(kernel='poly', degree=2, probability=True),\n",
    "    SVC(kernel='poly', degree=3, probability=True),  \n",
    "    SVC(kernel='poly', degree=4, probability=True),  \n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model_fit_and_evaluate(X_train_u, X_test_u, y_train_u, y_test_u, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b4e80",
   "metadata": {},
   "source": [
    "**Comparing the scores, we can clearly see that our feature transformation has clearly improved the performance of our models. We can easily understand it from the `SVC results`. Moreover, convergence is now faster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5884f9",
   "metadata": {},
   "source": [
    "#### Lets use some combined_features now if they can improve the baselines (~71%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0cb4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=False, degree=2):\n",
    "    steps = []\n",
    "\n",
    "    if use_poly:\n",
    "        steps.append(('poly', PolynomialFeatures(degree=degree)))\n",
    "    \n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    result =  {\n",
    "        'Model': model.__class__.__name__ + (f' (poly deg={degree})' if use_poly else ''),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC AUC': auc\n",
    "    }\n",
    "\n",
    "    if isinstance(model, SVC):\n",
    "        result.update({\n",
    "            'kernel': model.kernel,\n",
    "            'degree': model.degree,\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3edad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with Linear models....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:19<00:00, 69.81s/it]\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>kernel</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.721461</td>\n",
       "      <td>0.711712</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.721461</td>\n",
       "      <td>0.783262</td>\n",
       "      <td>poly</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.714612</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.716553</td>\n",
       "      <td>0.780197</td>\n",
       "      <td>linear</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.699115</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.714932</td>\n",
       "      <td>0.781636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.693966</td>\n",
       "      <td>0.745370</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.794680</td>\n",
       "      <td>rbf</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.689498</td>\n",
       "      <td>0.676991</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.768456</td>\n",
       "      <td>poly</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.682648</td>\n",
       "      <td>0.672646</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.683371</td>\n",
       "      <td>0.760761</td>\n",
       "      <td>poly</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression (poly deg=2)</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>0.646018</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.660633</td>\n",
       "      <td>0.697823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.655251</td>\n",
       "      <td>0.620818</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.688660</td>\n",
       "      <td>0.713536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC (poly deg=2)</td>\n",
       "      <td>0.646119</td>\n",
       "      <td>0.639269</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.676885</td>\n",
       "      <td>linear</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy  Precision    Recall  F1 Score  \\\n",
       "5                              SVC  0.721461   0.711712  0.731481  0.721461   \n",
       "2                              SVC  0.714612   0.702222  0.731481  0.716553   \n",
       "0               LogisticRegression  0.712329   0.699115  0.731481  0.714932   \n",
       "4                              SVC  0.712329   0.693966  0.745370  0.718750   \n",
       "6                              SVC  0.689498   0.676991  0.708333  0.692308   \n",
       "7                              SVC  0.682648   0.672646  0.694444  0.683371   \n",
       "1  LogisticRegression (poly deg=2)  0.657534   0.646018  0.675926  0.660633   \n",
       "8             KNeighborsClassifier  0.655251   0.620818  0.773148  0.688660   \n",
       "3                 SVC (poly deg=2)  0.646119   0.639269  0.648148  0.643678   \n",
       "\n",
       "    ROC AUC  kernel  degree  \n",
       "5  0.783262    poly     2.0  \n",
       "2  0.780197  linear     3.0  \n",
       "0  0.781636     NaN     NaN  \n",
       "4  0.794680     rbf     3.0  \n",
       "6  0.768456    poly     3.0  \n",
       "7  0.760761    poly     4.0  \n",
       "1  0.697823     NaN     NaN  \n",
       "8  0.713536     NaN     NaN  \n",
       "3  0.676885  linear     3.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Combined_feature was able to outperform the baseline (although by a mere margin). \n",
    "#   The current percent stands at ~ 72%\n",
    "\n",
    "results = []\n",
    "\n",
    "linear_models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    SVC(kernel='linear', probability=True)\n",
    "]\n",
    "\n",
    "nonlinear_models = [\n",
    "    SVC(kernel='rbf', probability=True),\n",
    "    SVC(kernel='poly', degree=2, probability=True),\n",
    "    SVC(kernel='poly', degree=3, probability=True),\n",
    "    SVC(kernel='poly', degree=4, probability=True),\n",
    "    KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "# Linear models with and without poly\n",
    "print('Starting with Linear models....')\n",
    "for model in tqdm(linear_models):\n",
    "    results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=False))     #   ~ 100 features\n",
    "    results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=True, degree=2))  # ~10000 features\n",
    "    # results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=True, degree=3))\n",
    "\n",
    "\n",
    "# Non-linear models without poly\n",
    "for model in tqdm(nonlinear_models):\n",
    "    results.append(model_fit_and_evaluate(X_train, X_test, y_train, y_test, model, use_poly=False))\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40459bfc",
   "metadata": {},
   "source": [
    "**Thus, among our baselines, `SVC` with `polynomial kernel` and `degree=2` provided us with the best results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772a93a",
   "metadata": {},
   "source": [
    "1. Now, we will move to the tree-based ensemble methods, such as RandomForest, XgBoost, LightGBM, CatBoost etc. An important thing to note now is that, for model-fitting on tree based methods, data transformations don't tend to work well. In fact, they are known to detoriate performace.\n",
    "\n",
    "2. That's why, we will be using the `final_df_not_qt` as our data here. Notably, the (numerical + other) features are not quantiled here. Also, the remaining features are also not standardized; perfect for fitting tree-based models.\n",
    "\n",
    "3. Also, it is not recommended to create polynomial features for tree-based ensembles. The models are capable of capturing interactions and nonlinearities on their own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480803d1",
   "metadata": {},
   "source": [
    "#### Performing train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7612fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 100) (438, 100) (1750,) (438,)\n"
     ]
    }
   ],
   "source": [
    "df = final_df_not_qt.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.20, shuffle=True, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc699ff",
   "metadata": {},
   "source": [
    "#### Tree-based ensemble model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455298a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tree_model(X_train, X_test, y_train, y_test, model, param_desc=''):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    return {\n",
    "        'Model': model.__class__.__name__,\n",
    "        'Params': param_desc,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'ROC AUC': auc\n",
    "    }\n",
    "\n",
    "def run_tree_model_grid(X_train, X_test, y_train, y_test):\n",
    "    results = []\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'n_estimators': [250, 450, 750],\n",
    "        'learning_rate': [0.05, 0.1],  # Only for boosting models\n",
    "    }\n",
    "\n",
    "    rf_params = [(d, n) for d in param_grid['max_depth'] for n in param_grid['n_estimators']]\n",
    "    boosting_params = [(d, n, lr) for d in param_grid['max_depth'] for n in param_grid['n_estimators'] for lr in param_grid['learning_rate']]\n",
    "\n",
    "    print(\"Running Random Forest...\")\n",
    "    for d, n in tqdm(rf_params):\n",
    "        model = RandomForestClassifier(max_depth=d, n_estimators=n, random_state=42)\n",
    "        desc = f'max_depth={d}, n_estimators={n}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    print(\"Running XGBoost...\")\n",
    "    for d, n, lr in tqdm(boosting_params):\n",
    "        model = xgb.XGBClassifier(max_depth=d, n_estimators=n, learning_rate=lr,\n",
    "                                   eval_metric='logloss', random_state=42)\n",
    "        desc = f'max_depth={d}, n_estimators={n}, lr={lr}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    print(\"Running LightGBM...\")\n",
    "    for d, n, lr in tqdm(boosting_params):\n",
    "        model = lgb.LGBMClassifier(max_depth=d, n_estimators=n, learning_rate=lr,\n",
    "                                force_col_wise=True, verbosity=-1, random_state=42)\n",
    "        desc = f'max_depth={d}, n_estimators={n}, lr={lr}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    print(\"Running CatBoost...\")\n",
    "    for d, n, lr in tqdm(boosting_params):\n",
    "        model = CatBoostClassifier(depth=d, iterations=n, learning_rate=lr,\n",
    "                                   verbose=0, random_state=42)\n",
    "        desc = f'depth={d}, iterations={n}, lr={lr}'\n",
    "        results.append(evaluate_tree_model(X_train, X_test, y_train, y_test, model, desc))\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55c5f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:16<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:41<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:27<00:00,  4.87s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Params</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>depth=3, iterations=250, lr=0.05</td>\n",
       "      <td>0.751142</td>\n",
       "      <td>0.735683</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.753950</td>\n",
       "      <td>0.813167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>max_depth=3, n_estimators=250, lr=0.05</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.736607</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.806953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>max_depth=7, n_estimators=250</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>0.810540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=3, n_estimators=250, lr=0.05</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>0.808037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>max_depth=5, n_estimators=450</td>\n",
       "      <td>0.746575</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.805535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=3, n_estimators=750, lr=0.1</td>\n",
       "      <td>0.710046</td>\n",
       "      <td>0.692641</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.715884</td>\n",
       "      <td>0.795796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=7, n_estimators=450, lr=0.1</td>\n",
       "      <td>0.707763</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.711712</td>\n",
       "      <td>0.799383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>depth=5, iterations=750, lr=0.1</td>\n",
       "      <td>0.707763</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.799383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=5, n_estimators=450, lr=0.1</td>\n",
       "      <td>0.705479</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.711409</td>\n",
       "      <td>0.795942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>max_depth=5, n_estimators=750, lr=0.1</td>\n",
       "      <td>0.700913</td>\n",
       "      <td>0.683983</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.706935</td>\n",
       "      <td>0.792918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model                                  Params  Accuracy  \\\n",
       "45      CatBoostClassifier        depth=3, iterations=250, lr=0.05  0.751142   \n",
       "9            XGBClassifier  max_depth=3, n_estimators=250, lr=0.05  0.748858   \n",
       "6   RandomForestClassifier           max_depth=7, n_estimators=250  0.748858   \n",
       "27          LGBMClassifier  max_depth=3, n_estimators=250, lr=0.05  0.748858   \n",
       "4   RandomForestClassifier           max_depth=5, n_estimators=450  0.746575   \n",
       "..                     ...                                     ...       ...   \n",
       "32          LGBMClassifier   max_depth=3, n_estimators=750, lr=0.1  0.710046   \n",
       "42          LGBMClassifier   max_depth=7, n_estimators=450, lr=0.1  0.707763   \n",
       "56      CatBoostClassifier         depth=5, iterations=750, lr=0.1  0.707763   \n",
       "36          LGBMClassifier   max_depth=5, n_estimators=450, lr=0.1  0.705479   \n",
       "38          LGBMClassifier   max_depth=5, n_estimators=750, lr=0.1  0.700913   \n",
       "\n",
       "    Precision    Recall  F1 Score   ROC AUC  \n",
       "45   0.735683  0.773148  0.753950  0.813167  \n",
       "9    0.736607  0.763889  0.750000  0.806953  \n",
       "6    0.734513  0.768519  0.751131  0.810540  \n",
       "27   0.734513  0.768519  0.751131  0.808037  \n",
       "4    0.733333  0.763889  0.748299  0.805535  \n",
       "..        ...       ...       ...       ...  \n",
       "32   0.692641  0.740741  0.715884  0.795796  \n",
       "42   0.692982  0.731481  0.711712  0.799383  \n",
       "56   0.703704  0.703704  0.703704  0.799383  \n",
       "36   0.688312  0.736111  0.711409  0.795942  \n",
       "38   0.683983  0.731481  0.706935  0.792918  \n",
       "\n",
       "[63 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tree_results = run_tree_model_grid(X_train, X_test, y_train, y_test)\n",
    "df_tree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b71d0",
   "metadata": {},
   "source": [
    "**From the above evaluations, it is clear that CatBoost outperformed the previous baseline and now the best performing model (CatBoost) stands at accuracy ~ 75% !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e958e33",
   "metadata": {},
   "source": [
    "1. Now, we will move to a new type of model, the Probabilistic models. We saw in the pre-processing section that after transformation, most of the (numerical + other) features were approximately Gaussian in their distribution.\n",
    "\n",
    "2. To fit these distributions, Gaussian Processes (GP) can be a good choice since they are non-parametric and based on Gaussian distributions. Along with predicting the class, they are also able to predict the uncertainity estimate for that prediction.\n",
    "\n",
    "3. We can use a hybrid kernel design (also combining with ARD kernel). This gives us natural feature selection and ability to handle both numerical and categorical features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring_Financial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
